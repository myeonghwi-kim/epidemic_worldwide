{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api key\n",
    "consumer_key \n",
    "consumer_secret \n",
    "access_token \n",
    "access_token_secret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#설정\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getKeywordList(): #KEYWORD 리스트 return\n",
    "    keyword = []\n",
    "\n",
    "    #WHO\n",
    "    keyword.append('Chikungunya')\n",
    "    keyword.append('Cholera')\n",
    "    #keyword.append('(Crimean AND Corgo)')\n",
    "    keyword.append('Ebola')\n",
    "    keyword.append('Hendra')\n",
    "    #keyword.append('(Lassa AND fever)')\n",
    "    #keyword.append('(pandemic AND influenza)')\n",
    "    #keyword.append('(seasonal AND influenza)')\n",
    "    #keyword.append('(zoonotic AND influenza)')\n",
    "    #keyword.append('(Marburg AND virus)')\n",
    "    keyword.append('Meningtis')\n",
    "    keyword.append('Mers')\n",
    "    keyword.append('Monkeypox')\n",
    "    keyword.append('Nipah')\n",
    "    keyword.append('Plague')\n",
    "    #keyword.append('(Rift AND Valley)')\n",
    "    keyword.append('SARS')\n",
    "    keyword.append('Smallpox')\n",
    "    keyword.append('Tularaemia')\n",
    "    #keyword.append('(Yellow AND fever)')\n",
    "    keyword.append('Zika')\n",
    "    #UN\n",
    "    keyword.append('Avain')\n",
    "    keyword.append('Dengue')\n",
    "    keyword.append('Malaria')\n",
    "    keyword.append('Measles')\n",
    "    #keyword.append('(Meningococcal AND Meningitis)')\n",
    "    keyword.append('AIDS')\n",
    "    keyword.append('HIV')\n",
    "    #General\n",
    "    keyword.append('Symptom')\n",
    "    keyword.append('Bacteria')\n",
    "    keyword.append('Vaccine')\n",
    "    keyword.append('Sickness')\n",
    "    keyword.append('Fever')\n",
    "    keyword.append('Cough')\n",
    "    keyword.append('Headache')\n",
    "    keyword.append('Chills')\n",
    "    keyword.append('Patient')\n",
    "    keyword.append('Myalgia')\n",
    "    keyword.append('Contagion')\n",
    "    keyword.append('Abdominal')\n",
    "    keyword.append('Diarrhea')\n",
    "    keyword.append('Hemorrhage')\n",
    "    keyword.append('Infection')\n",
    "    keyword.append('Arthralgia')\n",
    "    keyword.append('Vomiting')\n",
    "    keyword.append('Illness')\n",
    "    keyword.append('Inflammation')\n",
    "    keyword.append('Disease')\n",
    "    keyword.append('epidemicity')\n",
    "    return keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadTweet(nation): #TweetNoun return\n",
    "    TweetList = []\n",
    "    TweetNoun=[]\n",
    "    keyword = getKeywordList()\n",
    "    location = getLocation(nation)\n",
    "    for i in keyword:\n",
    "        cursor = tweepy.Cursor(api.search,\n",
    "                               q=i,\n",
    "                               fromDate=\n",
    "                               toDate=\n",
    "                               count=100,  # 페이지당 반환할 트위터 수 최대 100\n",
    "                               geocode=location + ',1000km',\n",
    "                               include_entities=True)\n",
    "        for tweet in cursor.items():\n",
    "            TweetList.append(tweet.text)\n",
    "        for k in TweetList:\n",
    "            tokens = nltk.word_tokenize(k)\n",
    "            tagged = nltk.pos_tag(tokens)\n",
    "            allnoun = [word for word, pos in tagged if pos in ['NN', 'NNS', 'NNP']]\n",
    "            for i in allnoun:\n",
    "                TweetNoun.append(i)\n",
    "\n",
    "    return TweetNoun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
